# Extraction de spans phÃ©notypiques avec des LLMs

Ce dÃ©pÃ´t contient des scripts pour **lâ€™extraction automatique de spans phÃ©notypiques** (et optionnellement la nÃ©gation) Ã  partir de phrases cliniques, en utilisant des **LLMs** :

- soit via un **appel local Python** (`transformers`, infÃ©rence dans le process),
- soit via un **serveur Ollama** (appel HTTP),
- avec une **configuration entiÃ¨rement externalisÃ©e** en JSON,
- et des sorties exploitables pour lâ€™Ã©valuation (format long TSV).

---

## ğŸ“ Arborescence du projet

```
.
â”œâ”€â”€ README.md
â”œâ”€â”€ configs
â”‚ â”œâ”€â”€ configs-local.json # Config infÃ©rence locale (Transformers)
â”‚ â”œâ”€â”€ configs-ollama.json # Config infÃ©rence via Ollama
â”‚ â””â”€â”€ obs # Anciennes configs / brouillons
â”œâ”€â”€ data
â”‚ â”œâ”€â”€ gold_sample_N500.tsv
â”‚ â””â”€â”€ gold_spans.tsv # Generated from CHU50
â”œâ”€â”€ notebook
â”‚ â”œâ”€â”€ evaluate_neg.ipynb
â”‚ â”œâ”€â”€ evaluate_span_detection.ipynb
â”‚ â””â”€â”€ evaluate_span_detection.py
â”œâ”€â”€ prompts
â”‚ â”œâ”€â”€ prompt.txt
â”‚ â”œâ”€â”€ small_models
â”‚ â”œâ”€â”€ span_prompt_with_examples.txt
â”‚ â”œâ”€â”€ span_prompt_with_examples_strict_version.txt
â”‚ â””â”€â”€ span_prompt_without_examples_strict_version.txt
â””â”€â”€ scripts
â”œâ”€â”€ run_spans.py # Script principal (local ou Ollama selon config)
â”œâ”€â”€ neg.py
â”œâ”€â”€ sample_gold.py
â”œâ”€â”€ prepare_gold_standard_for_span_detection_and_negation_evaluation.py
â”œâ”€â”€ evaluate_time_for_run_spans.py
â”œâ”€â”€ evaluate_time_for_run_spans_ollama.py
â””â”€â”€ evaluate_time_for_run_spans_vLLM.py
```

---

## ğŸ¯ FonctionnalitÃ©s principales

- Lecture dâ€™un dataset **1 phrase par ligne**
- Application de **plusieurs prompts** sur chaque phrase
- Appel :
  - soit dâ€™un **modÃ¨le local** (`transformers`)
  - soit dâ€™un **modÃ¨le distant via Ollama**
- Sortie au **format long** :
  - 1 ligne = 1 span prÃ©dit
  - conservation de la sortie brute du modÃ¨le
- Gestion :
  - des logs
  - de la reprise sur checkpoint
  - des temps dâ€™infÃ©rence

---

## âš™ï¸ PrÃ©-requis

### Python
- Python **3.9+** recommandÃ©

### DÃ©pendances minimales
```bash
pip install torch transformers accelerate pandas
```

âš ï¸ Pour les modÃ¨les volumineux (Qwen 32B, etc.), un GPU avec suffisamment de VRAM est fortement recommandÃ©.
Le paramÃ¨tre device_map="auto" est supportÃ©.


### ğŸ“„ Format des donnÃ©es en entrÃ©e

Le script lit un fichier TSV / CSV contenant une colonne de phrases.

Exemple minimal :
Sentence_en
Shortly after birth, he developed tachypnea...
MR spectroscopy showed a region of increased...

Le nom de la colonne est configurable via :

"io": {
  "sentence_col": "Sentence_en"
}

### â–¶ï¸ Utilisation
Lancer une extraction

`python scripts/run_spans.py --config configs/configs-local.json`

Ou avec Ollama :

`python scripts/run_spans.py --config configs/configs-ollama.json`

### ğŸ§© Configuration JSON

Toute la logique est pilotÃ©e par un fichier JSON.
Exemple : infÃ©rence locale (configs/configs-local.json)

```
{
  "paths": {
    "project_root": "/home/prollier/ext/Span_detection/",
    "models_root": "/home/prollier/models/",
    "data_root": "/home/prollier/output/for_span_detection_formatted/"
  },

  "model": {
    "model_name": "{models_root}/qwen3-32b",
    "device_map": "auto",
    "dtype": "float16",
    "local_files_only": true,
    "trust_remote_code": true
  },

  "generation": {
    "max_new_tokens": 64,
    "temperature": 0.0,
    "top_p": 1.0,
    "top_k": 50,
    "do_sample": false,
    "repetition_penalty": 1.0
  },

  "io": {
    "filename": "gold_sample_N500.tsv",
    "sep": "\t",
    "encoding": "utf-8",
    "sentence_col": "Sentence_en",
    "out_dir": "results"
  },

  "runtime": {
    "batch_size": 100,
    "log_file": "run_local.log",
    "log_level": "INFO",
    "resume": true
  },

  "prompt": {
    "template_paths": [
      "prompts/small_models/span_detection.txt",
      "prompts/small_models/span_detection_with_examples.txt"
    ],
    "sentence_var": "sentence"
  }
}
```

Exemple : infÃ©rence Ollama (configs/configs-ollama.json)

{
  "ollama": {
    "base_url": "https://compute-01.odh.local/ollama",
    "model": "deepseek-r1:8b-llama-distill-q4_K_M",
    "timeout_s": 120.0,
    "verify_ssl": false
  }
}

â¡ï¸ Cette config est combinÃ©e avec les sections communes (paths, io, prompt, etc.).

### ğŸ“¤ Format des sorties

Les rÃ©sultats sont Ã©crits dans :

results/spans_long_<model>.tsv

Colonnes importantes

    model
    prompt_name
    prompt_index
    span_index
    span_text
    spans_count
    raw_output
    latency_s

toutes les colonnes originales du dataset

ğŸ‘‰ Format long : une ligne par span (ou une ligne vide si aucun span).
ğŸ§ª Ã‰valuation

Les notebooks et scripts dâ€™Ã©valuation sont disponibles dans :

notebook/

    evaluate_span_detection.ipynb
    evaluate_neg.ipynb

Ils permettent de comparer les prÃ©dictions aux gold standards prÃ©sents dans data/.
ğŸ§  ModÃ¨les compatibles

    LLaMA / derivatives
    Meditron
    Qwen (souvent trust_remote_code=true)
    Tout modÃ¨le compatible AutoModelForCausalLM

ğŸš€ Extensions possibles

    Quantisation 4-bit / 8-bit (bitsandbytes)
    vLLM
    batching multi-phrases
    fallback automatique Ollama â†’ local
    parsing structurÃ© JSON strict

ğŸ“Œ Notes

    Aucun code nâ€™est spÃ©cifique Ã  une langue : EN / FR supportÃ©s
    Les prompts sont entiÃ¨rement externalisÃ©s
    Le script est conÃ§u pour des runs longs et reproductibles

ğŸ‘¤ Auteur / Contact
Projet interne â€” adaptÃ© pour lâ€™expÃ©rimentation LLM en extraction clinique.
