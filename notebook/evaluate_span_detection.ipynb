{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% [markdown]<br>\n",
    "# Evaluation multi-modèles de détection de spans<br>\n",
    "<br>\n",
    "- Entrée : fichiers de prédictions au format \"long\" (une ligne = 1 span prédit)<br>\n",
    "- Gold : colonnes gold_* déjà présentes dans ces fichiers<br>\n",
    "- Span-level : Levenshtein normalisé + seuil<br>\n",
    "- Agrégation : par modèle et par (modèle, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import re\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=========================\n",
    "1. Config\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motif des fichiers de sortie des différents modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTION_PATTERN = \"results/spans_long_*.tsv\"  # à adapter si besoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Séparateur des TSV (\",\" si tu as mis sep=\",\" dans ton script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEP = \",\"   # ou \"\\t\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seuil de similarité Levenshtein normalisée pour considérer un match comme TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEV_SIM_THRESHOLD = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colonnes clés pour identifier une phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY_COLS_SENTENCE = [\"report_id\", \"sentence_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nom de la colonne texte de la phrase (si tu veux l'afficher plus tard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_COL_CANDIDATES = [\"Sentence_en\", \"sentence\", \"Sentence\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nom de la colonne gold des spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_SPAN_COL = \"gold_span_text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nom de la colonne prédiction des spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRED_SPAN_COL = \"span_text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "2. Chargement des prédictions multi-modèles<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = sorted(glob.glob(PREDICTION_PATTERN))\n",
    "print(f\"Nb de fichiers de prédiction trouvés : {len(files)}\")\n",
    "for f in files:\n",
    "    print(\" -\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not files:\n",
    "    raise RuntimeError(\"Aucun fichier de prédiction trouvé : vérifie PREDICTION_PATTERN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for f in files:\n",
    "    df_tmp = pd.read_csv(f, sep=SEP)\n",
    "    df_tmp[\"source_file\"] = os.path.basename(f)\n",
    "    dfs.append(df_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.concat(dfs, ignore_index=True)\n",
    "print(\"Shape totale des prédictions :\", df_pred.shape)\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "3. Vérifications de colonnes<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "required_cols = set(KEY_COLS_SENTENCE + [GOLD_SPAN_COL, PRED_SPAN_COL, \"model\", \"prompt_name\", \"span_index\"])\n",
    "missing = required_cols - set(df_pred.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes dans df_pred : {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "colonne phrase (optionnelle mais pratique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_col = None\n",
    "for c in SENTENCE_COL_CANDIDATES:\n",
    "    if c in df_pred.columns:\n",
    "        sentence_col = c\n",
    "        break\n",
    "print(\"Colonne phrase utilisée :\", sentence_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "4. Normalisation de texte & Levenshtein<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_span(text: Any) -> str:\n",
    "    \"\"\"\n",
    "    Normalisation légère d'un span :\n",
    "    - lower\n",
    "    - strip\n",
    "    - réduction des espaces\n",
    "    - suppression ponctuation simple en bordure\n",
    "    \"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text)\n",
    "    s = s.lower()\n",
    "    s = s.strip()\n",
    "    # espaces multiples -> un seul\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    # enlever ponctuation simple aux extrémités\n",
    "    s = s.strip(\",.;:()[]{}\\\"'\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(a: str, b: str) -> int:\n",
    "    \"\"\"\n",
    "    Distance de Levenshtein (édition) sur caractères.\n",
    "    Implémentation simple O(len(a)*len(b)).\n",
    "    \"\"\"\n",
    "    if a == b:\n",
    "        return 0\n",
    "    la, lb = len(a), len(b)\n",
    "    if la == 0:\n",
    "        return lb\n",
    "    if lb == 0:\n",
    "        return la\n",
    "\n",
    "    # DP sur une seule dimension\n",
    "    prev_row = list(range(lb + 1))\n",
    "    for i, ca in enumerate(a, start=1):\n",
    "        curr_row = [i]\n",
    "        for j, cb in enumerate(b, start=1):\n",
    "            insert_cost = curr_row[j - 1] + 1\n",
    "            delete_cost = prev_row[j] + 1\n",
    "            replace_cost = prev_row[j - 1] + (ca != cb)\n",
    "            curr_row.append(min(insert_cost, delete_cost, replace_cost))\n",
    "        prev_row = curr_row\n",
    "    return prev_row[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Similarité Levenshtein normalisée dans [0,1].\n",
    "    sim = 1 - lev / max_len\n",
    "    \"\"\"\n",
    "    a_norm = normalize_span(a)\n",
    "    b_norm = normalize_span(b)\n",
    "    if not a_norm and not b_norm:\n",
    "        return 1.0\n",
    "    if not a_norm or not b_norm:\n",
    "        return 0.0\n",
    "    d = levenshtein_distance(a_norm, b_norm)\n",
    "    max_len = max(len(a_norm), len(b_norm))\n",
    "    return 1.0 - d / max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Petit test rapide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sim('hypotonie', 'hypotonia') =\", levenshtein_similarity(\"hypotonie\", \"hypotonia\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "5. Construction des gold spans par phrase<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On suppose que les colonnes gold_* sont déjà présentes dans df_pred (copiées du dataset d'origine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df = df_pred[KEY_COLS_SENTENCE + [GOLD_SPAN_COL]].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalisation et filtrage des gold spans non vides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_df[\"gold_span_norm\"] = gold_df[GOLD_SPAN_COL].apply(normalize_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_gold_spans(group: pd.DataFrame) -> List[str]:\n",
    "    spans = [s for s in group[\"gold_span_norm\"].tolist() if s]\n",
    "    # uniq pour éviter les doublons\n",
    "    return sorted(set(spans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_by_sent: Dict[Tuple, List[str]] = (\n",
    "    gold_df\n",
    "    .groupby(KEY_COLS_SENTENCE, dropna=False)\n",
    "    .apply(collect_gold_spans)\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_sents_with_gold = sum(1 for v in gold_by_sent.values() if len(v) > 0)\n",
    "print(\"Nb total de phrases :\", len(gold_by_sent))\n",
    "print(\"Nb de phrases avec au moins un gold span :\", nb_sents_with_gold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "6. Construction des spans prédits par phrase / modèle / prompt<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour les spans prédits, on ignore les lignes sentinel avec span_index = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_spans = df_pred[df_pred[\"span_index\"] >= 0].copy()\n",
    "df_pred_spans[\"pred_span_norm\"] = df_pred_spans[PRED_SPAN_COL].apply(normalize_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_pred_spans(group: pd.DataFrame) -> List[str]:\n",
    "    spans = [s for s in group[\"pred_span_norm\"].tolist() if s]\n",
    "    return spans  # on ne déduplique pas forcément ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dict: (model, prompt_name, report_id, sentence_id) -> [pred spans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_by_model_prompt_sent: Dict[Tuple, List[str]] = (\n",
    "    df_pred_spans\n",
    "    .groupby([\"model\", \"prompt_name\"] + KEY_COLS_SENTENCE, dropna=False)\n",
    "    .apply(collect_pred_spans)\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pred_by_model_prompt_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "7. Fonction de comptage TP / FP / FN (span-level, Levenshtein)<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tp_fp_fn_for_sentence(\n",
    "    gold_spans: List[str],\n",
    "    pred_spans: List[str],\n",
    "    threshold: float,\n",
    ") -> Tuple[int, int, int]:\n",
    "    \"\"\"\n",
    "    Compte TP, FP, FN pour une phrase donnée (ensemble de gold_spans et de pred_spans)\n",
    "    avec matching basé sur similarité Levenshtein >= threshold.\n",
    "    NB : pas de matching one-to-one (Hungarian) => une même gold peut être utilisée\n",
    "    pour plusieurs prédictions (on suit ta consigne \"no Hungarian\").\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "\n",
    "    # TP / FP (côté prédictions)\n",
    "    for pred in pred_spans:\n",
    "        if gold_spans and any(levenshtein_similarity(pred, g) >= threshold for g in gold_spans):\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "\n",
    "    # FN (côté gold)\n",
    "    for gold in gold_spans:\n",
    "        if not pred_spans or all(levenshtein_similarity(pred, gold) < threshold for pred in pred_spans):\n",
    "            fn += 1\n",
    "    return tp, fp, fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "8. Evaluation span-level par (modèle, prompt)<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_span_level_for_model_prompt(\n",
    "    df: pd.DataFrame,\n",
    "    gold_by_sent: Dict[Tuple, List[str]],\n",
    "    threshold: float = 0.8,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    df : dataFrame complet des prédictions (multi-modèles)\n",
    "    gold_by_sent : dict (report_id, sentence_id) -> [gold spans]\n",
    "    Retourne un DataFrame de métriques par (model, prompt_name).\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    # On re-filtre les spans prédits ici pour être sûr\n",
    "    df_spans = df[df[\"span_index\"] >= 0].copy()\n",
    "    df_spans[\"pred_span_norm\"] = df_spans[PRED_SPAN_COL].apply(normalize_span)\n",
    "\n",
    "    # Boucle sur chaque (modèle, prompt)\n",
    "    for (model, prompt_name), df_mp in df_spans.groupby([\"model\", \"prompt_name\"]):\n",
    "        # dict local : (report_id, sentence_id) -> [pred spans]\n",
    "        local_pred_by_sent = (\n",
    "            df_mp\n",
    "            .groupby(KEY_COLS_SENTENCE, dropna=False)[\"pred_span_norm\"]\n",
    "            .apply(list)\n",
    "            .to_dict()\n",
    "        )\n",
    "        tp_total, fp_total, fn_total = 0, 0, 0\n",
    "\n",
    "        # On considère toutes les phrases présentes dans le gold (pour comptage FN),\n",
    "        # même si le modèle n'a rien prédit dessus\n",
    "        for sent_key, gold_spans in gold_by_sent.items():\n",
    "            pred_spans = local_pred_by_sent.get(sent_key, [])\n",
    "            tp, fp, fn = count_tp_fp_fn_for_sentence(gold_spans, pred_spans, threshold)\n",
    "            tp_total += tp\n",
    "            fp_total += fp\n",
    "            fn_total += fn\n",
    "        precision = tp_total / (tp_total + fp_total) if (tp_total + fp_total) > 0 else 0.0\n",
    "        recall = tp_total / (tp_total + fn_total) if (tp_total + fn_total) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        results.append({\n",
    "            \"model\": model,\n",
    "            \"prompt_name\": prompt_name,\n",
    "            \"tp\": tp_total,\n",
    "            \"fp\": fp_total,\n",
    "            \"fn\": fn_total,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        })\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_mp = evaluate_span_level_for_model_prompt(df_pred, gold_by_sent, threshold=LEV_SIM_THRESHOLD)\n",
    "df_metrics_mp.sort_values(\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "9. Agrégation par modèle (en moyennant sur les prompts)<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agrégation simple : on recompute TP/FP/FN par modèle (somme sur prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg = (\n",
    "    df_metrics_mp\n",
    "    .groupby(\"model\", as_index=False)[[\"tp\", \"fp\", \"fn\"]]\n",
    "    .sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg[\"precision\"] = agg[\"tp\"] / (agg[\"tp\"] + agg[\"fp\"])\n",
    "agg[\"recall\"] = agg[\"tp\"] / (agg[\"tp\"] + agg[\"fn\"])\n",
    "agg[\"f1\"] = 2 * agg[\"precision\"] * agg[\"recall\"] / (agg[\"precision\"] + agg[\"recall\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics_model = agg.sort_values(\"f1\", ascending=False)\n",
    "df_metrics_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "10. Visualisation : F1 par modèle<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "order = df_metrics_model.sort_values(\"f1\")[\"model\"].tolist()\n",
    "plt.barh(df_metrics_model[\"model\"], df_metrics_model[\"f1\"])\n",
    "plt.xlabel(\"F1 (span-level, Levenshtein)\")\n",
    "plt.title(\"Comparaison des modèles (tous prompts confondus)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "11. Visualisation : F1 par (modèle, prompt)<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "x_labels = [f\"{m}\\n{p}\" for m, p in zip(df_metrics_mp[\"model\"], df_metrics_mp[\"prompt_name\"])]\n",
    "plt.bar(range(len(df_metrics_mp)), df_metrics_mp[\"f1\"])\n",
    "plt.xticks(range(len(df_metrics_mp)), x_labels, rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"F1 (span-level, Levenshtein)\")\n",
    "plt.title(\"Comparaison modèles / prompts\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%<br>\n",
    "=========================<br>\n",
    "12. Inspection d'exemples (optionnel)<br>\n",
    "========================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemple : filtrer quelques phrases où un modèle donné a des FP ou FN (à inspecter à la main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_INSPECT = df_metrics_model.iloc[0][\"model\"]  # meilleur modèle par défaut\n",
    "PROMPT_TO_INSPECT = df_metrics_mp.sort_values(\"f1\", ascending=False).iloc[0][\"prompt_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Inspection pour :\", MODEL_TO_INSPECT, \"/\", PROMPT_TO_INSPECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp = df_pred[(df_pred[\"model\"] == MODEL_TO_INSPECT) & (df_pred[\"prompt_name\"] == PROMPT_TO_INSPECT)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrases avec gold spans mais aucune prédiction (FN typiques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_with_gold = [k for k, v in gold_by_sent.items() if len(v) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recalcule dictionnaire locaux (comme plus haut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mp_spans = df_mp[df_mp[\"span_index\"] >= 0].copy()\n",
    "df_mp_spans[\"pred_span_norm\"] = df_mp_spans[PRED_SPAN_COL].apply(normalize_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_by_sent_local = (\n",
    "    df_mp_spans\n",
    "    .groupby(KEY_COLS_SENTENCE)[\"pred_span_norm\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn_sentences = []\n",
    "for sent_key in sent_with_gold:\n",
    "    gold_spans = gold_by_sent.get(sent_key, [])\n",
    "    pred_spans = pred_by_sent_local.get(sent_key, [])\n",
    "    _, _, fn = count_tp_fp_fn_for_sentence(gold_spans, pred_spans, LEV_SIM_THRESHOLD)\n",
    "    if fn > 0:\n",
    "        fn_sentences.append((sent_key, gold_spans, pred_spans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Nb de phrases avec au moins un FN pour ce modèle/prompt : {len(fn_sentences)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichage de quelques exemples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (report_id, sentence_id), gold_spans, pred_spans in fn_sentences[:5]:\n",
    "    row_example = df_mp[(df_mp[\"report_id\"] == report_id) & (df_mp[\"sentence_id\"] == sentence_id)].iloc[0]\n",
    "    sent_text = row_example.get(sentence_col, \"[[no sentence col]]\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Report: {report_id} | sentence_id: {sentence_id}\")\n",
    "    print(\"Sentence:\", sent_text)\n",
    "    print(\"Gold spans:\", gold_spans)\n",
    "    print(\"Pred spans:\", pred_spans)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
