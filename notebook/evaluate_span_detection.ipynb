{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2090b9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6555394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 fichiers trouvés\n",
      " - simulated_spans_falcon7b.tsv\n",
      " - simulated_spans_qwen3-32b.tsv\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR = \"../results/\"   # ou \"output\"\n",
    "SEP = \"\\t\"\n",
    "\n",
    "files = sorted(glob.glob(os.path.join(OUTPUT_DIR, \"simulated_spans_*.tsv\"))) # spans_long_*\n",
    "\n",
    "print(f\"{len(files)} fichiers trouvés\")\n",
    "for f in files:\n",
    "    print(\" -\", Path(f).name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36b27219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape globale : (5160, 18)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f, sep=SEP)\n",
    "    df[\"source_file\"] = Path(f).name\n",
    "    dfs.append(df)\n",
    "\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "print(\"Shape globale :\", df_all.shape)\n",
    "# df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2817636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.copy()\n",
    "\n",
    "# Prédiction binaire : y a-t-il AU MOINS un span pour ce prompt / phrase ?\n",
    "df[\"pred_annotation\"] = (df[\"spans_count\"] > 0).astype(int)\n",
    "\n",
    "# Span non vide\n",
    "df[\"has_span\"] = df[\"span_index\"] >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "aa34f4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/hw7c94m14pn9syslmn86py080000gq/T/ipykernel_44480/4218217163.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(eval_annotation)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>ann_accuracy</th>\n",
       "      <th>ann_precision</th>\n",
       "      <th>ann_recall</th>\n",
       "      <th>ann_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>span_detection_with_examples</td>\n",
       "      <td>0.569575</td>\n",
       "      <td>0.604076</td>\n",
       "      <td>0.816929</td>\n",
       "      <td>0.694561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>span_detection_with_examples</td>\n",
       "      <td>0.569575</td>\n",
       "      <td>0.604076</td>\n",
       "      <td>0.816929</td>\n",
       "      <td>0.694561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>negation_detection</td>\n",
       "      <td>0.552204</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.822394</td>\n",
       "      <td>0.688207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>negation_detection</td>\n",
       "      <td>0.552204</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.822394</td>\n",
       "      <td>0.688207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>span_detection</td>\n",
       "      <td>0.548276</td>\n",
       "      <td>0.580110</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.681265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>span_detection</td>\n",
       "      <td>0.548276</td>\n",
       "      <td>0.580110</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.681265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model                   prompt_name  ann_accuracy  ann_precision  \\\n",
       "2   falcon7b  span_detection_with_examples      0.569575       0.604076   \n",
       "5  qwen3-32b  span_detection_with_examples      0.569575       0.604076   \n",
       "0   falcon7b            negation_detection      0.552204       0.591667   \n",
       "3  qwen3-32b            negation_detection      0.552204       0.591667   \n",
       "1   falcon7b                span_detection      0.548276       0.580110   \n",
       "4  qwen3-32b                span_detection      0.548276       0.580110   \n",
       "\n",
       "   ann_recall    ann_f1  \n",
       "2    0.816929  0.694561  \n",
       "5    0.816929  0.694561  \n",
       "0    0.822394  0.688207  \n",
       "3    0.822394  0.688207  \n",
       "1    0.825147  0.681265  \n",
       "4    0.825147  0.681265  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_annotation(df_sub):\n",
    "    y_true = df_sub[\"gold_annotation\"]\n",
    "    y_pred = df_sub[\"pred_annotation\"]\n",
    "\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "\n",
    "    return pd.Series({\n",
    "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "        \"precision\": p,\n",
    "        \"recall\": r,\n",
    "        \"f1\": f1,\n",
    "    })\n",
    "\n",
    "\n",
    "annotation_scores = (\n",
    "    df\n",
    "    .groupby([\"model\", \"prompt_name\"])\n",
    "    .apply(eval_annotation)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "annotation_scores_renamed = annotation_scores.rename(columns={\n",
    "    \"accuracy\": \"ann_accuracy\",\n",
    "    \"precision\": \"ann_precision\",\n",
    "    \"recall\": \"ann_recall\",\n",
    "    \"f1\": \"ann_f1\",\n",
    "})\n",
    "\n",
    "\n",
    "annotation_scores_renamed.sort_values(\"ann_f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2908b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEGATION_PROMPT_KEYWORDS = [\"negation\"]\n",
    "\n",
    "mask_neg_prompt = df[\"prompt_name\"].str.contains(\n",
    "    \"|\".join(NEGATION_PROMPT_KEYWORDS), case=False\n",
    ")\n",
    "\n",
    "df_neg = df[mask_neg_prompt & df[\"has_span\"]].copy()\n",
    "\n",
    "df_neg[\"pred_negated\"] = (\n",
    "    df_neg[\"span_text\"].str.contains(\"NOT_|NEG_\", regex=True, na=False)\n",
    ").astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b60df0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6n/hw7c94m14pn9syslmn86py080000gq/T/ipykernel_44480/3032351268.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>neg_precision</th>\n",
       "      <th>neg_recall</th>\n",
       "      <th>neg_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>negation_detection</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>negation_detection</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model         prompt_name  neg_precision  neg_recall  neg_f1\n",
       "0   falcon7b  negation_detection            0.0         0.0     0.0\n",
       "1  qwen3-32b  negation_detection            0.0         0.0     0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_scores = (\n",
    "    df_neg\n",
    "    .groupby([\"model\", \"prompt_name\"])\n",
    "    .apply(\n",
    "        lambda x: precision_recall_fscore_support(\n",
    "            x[\"gold_negated\"],\n",
    "            x[\"pred_negated\"],\n",
    "            average=\"binary\",\n",
    "            zero_division=0\n",
    "        )[:3]\n",
    "    )\n",
    ")\n",
    "\n",
    "neg_scores = neg_scores.apply(pd.Series)\n",
    "neg_scores.columns = [\"precision\", \"recall\", \"f1\"]\n",
    "neg_scores = neg_scores.reset_index()\n",
    "\n",
    "\n",
    "neg_scores_renamed = neg_scores.rename(columns={\n",
    "    \"precision\": \"neg_precision\",\n",
    "    \"recall\": \"neg_recall\",\n",
    "    \"f1\": \"neg_f1\",\n",
    "})\n",
    "\n",
    "\n",
    "neg_scores_renamed.sort_values(\"neg_f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "07e6adf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(a: str, b: str) -> int:\n",
    "    \"\"\"\n",
    "    Distance de Levenshtein classique (programmation dynamique).\n",
    "    \"\"\"\n",
    "    a = str(a)\n",
    "    b = str(b)\n",
    "\n",
    "    if a == b:\n",
    "        return 0\n",
    "    if len(a) == 0:\n",
    "        return len(b)\n",
    "    if len(b) == 0:\n",
    "        return len(a)\n",
    "\n",
    "    dp = [[0] * (len(b) + 1) for _ in range(len(a) + 1)]\n",
    "\n",
    "    for i in range(len(a) + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len(b) + 1):\n",
    "        dp[0][j] = j\n",
    "\n",
    "    for i in range(1, len(a) + 1):\n",
    "        for j in range(1, len(b) + 1):\n",
    "            cost = 0 if a[i - 1] == b[j - 1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,      # deletion\n",
    "                dp[i][j - 1] + 1,      # insertion\n",
    "                dp[i - 1][j - 1] + cost  # substitution\n",
    "            )\n",
    "\n",
    "    return dp[-1][-1]\n",
    "\n",
    "\n",
    "def levenshtein_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"\n",
    "    Similarité Levenshtein normalisée entre 0 et 1.\n",
    "    \"\"\"\n",
    "    a = str(a).strip().lower()\n",
    "    b = str(b).strip().lower()\n",
    "\n",
    "    if not a and not b:\n",
    "        return 1.0\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "\n",
    "    dist = levenshtein_distance(a, b)\n",
    "    return 1.0 - dist / max(len(a), len(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f9e56232",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gold_span_text</th>\n",
       "      <th>span_text</th>\n",
       "      <th>lev_sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en période néo-natale notamment pas de notion ...</td>\n",
       "      <td>fever</td>\n",
       "      <td>0.035088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>trouble du spectre autistique</td>\n",
       "      <td>fever</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>trouble du spectre autistique</td>\n",
       "      <td>respiratory distress</td>\n",
       "      <td>0.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>trouble du spectre autistique</td>\n",
       "      <td>respiratory distress</td>\n",
       "      <td>0.275862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>trouble du spectre autistique</td>\n",
       "      <td>fever</td>\n",
       "      <td>0.103448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       gold_span_text             span_text  \\\n",
       "3   en période néo-natale notamment pas de notion ...                 fever   \n",
       "9                       trouble du spectre autistique                 fever   \n",
       "10                      trouble du spectre autistique  respiratory distress   \n",
       "11                      trouble du spectre autistique  respiratory distress   \n",
       "12                      trouble du spectre autistique                 fever   \n",
       "\n",
       "     lev_sim  \n",
       "3   0.035088  \n",
       "9   0.103448  \n",
       "10  0.275862  \n",
       "11  0.275862  \n",
       "12  0.103448  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_span = df[df[\"gold_span_text\"].notna() & df[\"has_span\"]].copy()\n",
    "\n",
    "df_span[\"lev_sim\"] = df_span.apply(\n",
    "    lambda r: levenshtein_similarity(r[\"gold_span_text\"], r[\"span_text\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_span[[\n",
    "    \"gold_span_text\",\n",
    "    \"span_text\",\n",
    "    \"lev_sim\"\n",
    "]].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ee022750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>lev_mean</th>\n",
       "      <th>lev_median</th>\n",
       "      <th>lev_p75</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>span_detection</td>\n",
       "      <td>0.140016</td>\n",
       "      <td>0.129331</td>\n",
       "      <td>0.180070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>span_detection</td>\n",
       "      <td>0.140016</td>\n",
       "      <td>0.129331</td>\n",
       "      <td>0.180070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>span_detection_with_examples</td>\n",
       "      <td>0.131801</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.170892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>span_detection_with_examples</td>\n",
       "      <td>0.131801</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.170892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>negation_detection</td>\n",
       "      <td>0.131121</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>negation_detection</td>\n",
       "      <td>0.131121</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model                   prompt_name  lev_mean  lev_median   lev_p75\n",
       "1   falcon7b                span_detection  0.140016    0.129331  0.180070\n",
       "4  qwen3-32b                span_detection  0.140016    0.129331  0.180070\n",
       "2   falcon7b  span_detection_with_examples  0.131801    0.127273  0.170892\n",
       "5  qwen3-32b  span_detection_with_examples  0.131801    0.127273  0.170892\n",
       "0   falcon7b            negation_detection  0.131121    0.125000  0.166667\n",
       "3  qwen3-32b            negation_detection  0.131121    0.125000  0.166667"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lev_scores = (\n",
    "    df_span\n",
    "    .groupby([\"model\", \"prompt_name\"])\n",
    "    .agg(\n",
    "        lev_sim_mean=(\"lev_sim\", \"mean\"),\n",
    "        lev_sim_median=(\"lev_sim\", \"median\"),\n",
    "        lev_sim_p75=(\"lev_sim\", lambda x: x.quantile(0.75)),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values(\"lev_sim_mean\", ascending=False)\n",
    ")\n",
    "\n",
    "lev_scores_renamed = lev_scores.rename(columns={\n",
    "    \"lev_sim_mean\": \"lev_mean\",\n",
    "    \"lev_sim_median\": \"lev_median\",\n",
    "    \"lev_sim_p75\": \"lev_p75\",\n",
    "})\n",
    "\n",
    "\n",
    "lev_scores_renamed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "29608d30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_name</th>\n",
       "      <th>n_sentences</th>\n",
       "      <th>pct_with_span</th>\n",
       "      <th>mean_spans</th>\n",
       "      <th>mean_latency_s</th>\n",
       "      <th>ann_accuracy</th>\n",
       "      <th>ann_precision</th>\n",
       "      <th>ann_recall</th>\n",
       "      <th>ann_f1</th>\n",
       "      <th>lev_mean</th>\n",
       "      <th>lev_median</th>\n",
       "      <th>lev_p75</th>\n",
       "      <th>neg_precision</th>\n",
       "      <th>neg_recall</th>\n",
       "      <th>neg_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>negation_detection</td>\n",
       "      <td>442</td>\n",
       "      <td>0.835267</td>\n",
       "      <td>1.955916</td>\n",
       "      <td>0.903788</td>\n",
       "      <td>0.552204</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.822394</td>\n",
       "      <td>0.688207</td>\n",
       "      <td>0.131121</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>span_detection</td>\n",
       "      <td>442</td>\n",
       "      <td>0.832184</td>\n",
       "      <td>1.972414</td>\n",
       "      <td>0.893844</td>\n",
       "      <td>0.548276</td>\n",
       "      <td>0.580110</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.681265</td>\n",
       "      <td>0.140016</td>\n",
       "      <td>0.129331</td>\n",
       "      <td>0.180070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>falcon7b</td>\n",
       "      <td>span_detection_with_examples</td>\n",
       "      <td>442</td>\n",
       "      <td>0.810142</td>\n",
       "      <td>1.885613</td>\n",
       "      <td>0.891175</td>\n",
       "      <td>0.569575</td>\n",
       "      <td>0.604076</td>\n",
       "      <td>0.816929</td>\n",
       "      <td>0.694561</td>\n",
       "      <td>0.131801</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.170892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>negation_detection</td>\n",
       "      <td>442</td>\n",
       "      <td>0.835267</td>\n",
       "      <td>1.955916</td>\n",
       "      <td>0.903788</td>\n",
       "      <td>0.552204</td>\n",
       "      <td>0.591667</td>\n",
       "      <td>0.822394</td>\n",
       "      <td>0.688207</td>\n",
       "      <td>0.131121</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>span_detection</td>\n",
       "      <td>442</td>\n",
       "      <td>0.832184</td>\n",
       "      <td>1.972414</td>\n",
       "      <td>0.893844</td>\n",
       "      <td>0.548276</td>\n",
       "      <td>0.580110</td>\n",
       "      <td>0.825147</td>\n",
       "      <td>0.681265</td>\n",
       "      <td>0.140016</td>\n",
       "      <td>0.129331</td>\n",
       "      <td>0.180070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>qwen3-32b</td>\n",
       "      <td>span_detection_with_examples</td>\n",
       "      <td>442</td>\n",
       "      <td>0.810142</td>\n",
       "      <td>1.885613</td>\n",
       "      <td>0.891175</td>\n",
       "      <td>0.569575</td>\n",
       "      <td>0.604076</td>\n",
       "      <td>0.816929</td>\n",
       "      <td>0.694561</td>\n",
       "      <td>0.131801</td>\n",
       "      <td>0.127273</td>\n",
       "      <td>0.170892</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model                   prompt_name  n_sentences  pct_with_span  \\\n",
       "0   falcon7b            negation_detection          442       0.835267   \n",
       "1   falcon7b                span_detection          442       0.832184   \n",
       "2   falcon7b  span_detection_with_examples          442       0.810142   \n",
       "3  qwen3-32b            negation_detection          442       0.835267   \n",
       "4  qwen3-32b                span_detection          442       0.832184   \n",
       "5  qwen3-32b  span_detection_with_examples          442       0.810142   \n",
       "\n",
       "   mean_spans  mean_latency_s  ann_accuracy  ann_precision  ann_recall  \\\n",
       "0    1.955916        0.903788      0.552204       0.591667    0.822394   \n",
       "1    1.972414        0.893844      0.548276       0.580110    0.825147   \n",
       "2    1.885613        0.891175      0.569575       0.604076    0.816929   \n",
       "3    1.955916        0.903788      0.552204       0.591667    0.822394   \n",
       "4    1.972414        0.893844      0.548276       0.580110    0.825147   \n",
       "5    1.885613        0.891175      0.569575       0.604076    0.816929   \n",
       "\n",
       "     ann_f1  lev_mean  lev_median   lev_p75  neg_precision  neg_recall  neg_f1  \n",
       "0  0.688207  0.131121    0.125000  0.166667            0.0         0.0     0.0  \n",
       "1  0.681265  0.140016    0.129331  0.180070            NaN         NaN     NaN  \n",
       "2  0.694561  0.131801    0.127273  0.170892            NaN         NaN     NaN  \n",
       "3  0.688207  0.131121    0.125000  0.166667            0.0         0.0     0.0  \n",
       "4  0.681265  0.140016    0.129331  0.180070            NaN         NaN     NaN  \n",
       "5  0.694561  0.131801    0.127273  0.170892            NaN         NaN     NaN  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview = (\n",
    "    df\n",
    "    .groupby([\"model\", \"prompt_name\"])\n",
    "    .agg(\n",
    "        n_sentences=(\"sentence\", \"nunique\"),\n",
    "        mean_spans=(\"spans_count\", \"mean\"),\n",
    "        pct_with_span=(\"pred_annotation\", \"mean\"),\n",
    "        mean_latency_s=(\"latency_s\", \"mean\"),\n",
    "    )\n",
    "    .reset_index()\n",
    "    .sort_values([\"model\", \"prompt_name\"])\n",
    ")\n",
    "\n",
    "overview\n",
    "\n",
    "\n",
    "summary = (\n",
    "    overview\n",
    "    .merge(annotation_scores_renamed, on=[\"model\", \"prompt_name\"], how=\"left\")\n",
    "    .merge(lev_scores_renamed, on=[\"model\", \"prompt_name\"], how=\"left\")\n",
    "    .merge(neg_scores_renamed, on=[\"model\", \"prompt_name\"], how=\"left\")\n",
    ")\n",
    "\n",
    "cols = [\n",
    "    \"model\",\"prompt_name\",\n",
    "    \"n_sentences\",\"pct_with_span\",\"mean_spans\",\"mean_latency_s\",\n",
    "    \"ann_accuracy\",\"ann_precision\",\"ann_recall\",\"ann_f1\",\n",
    "    \"lev_mean\",\"lev_median\",\"lev_p75\",\n",
    "    \"neg_precision\",\"neg_recall\",\"neg_f1\",\n",
    "]\n",
    "summary = summary[[c for c in cols if c in summary.columns]]\n",
    "summary\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cluster_analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
